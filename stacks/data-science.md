# Data Science Development

## ğŸ“‹ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

```bash
# Jupyterç’°å¢ƒèµ·å‹•ï¼ˆ99%ä½¿ç”¨ï¼‰
jupyter lab              # Jupyter Labèµ·å‹•ï¼ˆæ¨å¥¨ï¼‰
jupyter notebook         # Jupyter Notebookèµ·å‹•

# Pythonç’°å¢ƒï¼ˆ95%ä½¿ç”¨ï¼‰
python script.py         # ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
ipython                  # å¯¾è©±å‹Python

# å®Ÿé¨“ç®¡ç†ï¼ˆ85%ä½¿ç”¨ï¼‰
mlflow ui                # MLflow UIèµ·å‹•
dvc pull                 # ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
wandb login              # Weights & Biasesé€£æº
```

## ğŸ¯ å“è³ªåŸºæº–

### ã‚³ãƒ¼ãƒ‰å“è³ª
- **Type Hints**: é–¢æ•°ã‚·ã‚°ãƒãƒãƒ£ã«å‹ãƒ’ãƒ³ãƒˆ
- **Docstrings**: é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- **Linter**: flake8ã€pylintã§0ã‚¨ãƒ©ãƒ¼
- **Formatter**: blackã§çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

### å†ç¾æ€§
- **ç’°å¢ƒç®¡ç†**: conda/venv + requirements.txt
- **ä¹±æ•°å›ºå®š**: random_state/seedã®æ˜ç¤ºçš„è¨­å®š
- **ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°**: DVCä½¿ç”¨
- **å®Ÿé¨“ç®¡ç†**: MLflowã€Weights & Biases

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- **README**: ç’°å¢ƒæ§‹ç¯‰æ‰‹é †ã€å®Ÿè¡Œæ–¹æ³•
- **Notebook**: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚»ãƒ«ã§èª¬æ˜
- **çµæœè¨˜éŒ²**: å®Ÿé¨“çµæœã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹

## ğŸ”’ ãƒ‡ãƒ¼ã‚¿ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

### å€‹äººæƒ…å ±ä¿è­·
- **åŒ¿ååŒ–**: å€‹äººè­˜åˆ¥æƒ…å ±ã®å‰Šé™¤ãƒ»ãƒã‚¹ã‚¯
- **ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡**: ãƒ‡ãƒ¼ã‚¿ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ç®¡ç†
- **æš—å·åŒ–**: æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã®æš—å·åŒ–ä¿å­˜

### ãƒ‡ãƒ¼ã‚¿ç®¡ç†
```python
# æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨ãªèª­ã¿è¾¼ã¿
import os
from dotenv import load_dotenv

load_dotenv()  # .envã‹ã‚‰èª­ã¿è¾¼ã¿
DB_PASSWORD = os.getenv('DB_PASSWORD')  # ç’°å¢ƒå¤‰æ•°ä½¿ç”¨

# âŒ ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ç¦æ­¢
# DB_PASSWORD = "secret123"
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- **pandas**: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œã€vectorizationæ´»ç”¨
- **numpy**: æ•°å€¤è¨ˆç®—ã€è¡Œåˆ—æ¼”ç®—
- **dask**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—å‡¦ç†
- **polars**: é«˜é€ŸDataFrameï¼ˆRustå®Ÿè£…ï¼‰

### ãƒ¡ãƒ¢ãƒªç®¡ç†
```python
import pandas as pd

# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªèª­ã¿è¾¼ã¿
df = pd.read_csv('large_file.csv',
    dtype={'col1': 'int32'},  # å‹æŒ‡å®šã§å‰Šæ¸›
    usecols=['col1', 'col2'],  # å¿…è¦åˆ—ã®ã¿
    chunksize=10000  # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿
)

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
print(df.memory_usage(deep=True))
```

## ğŸ’¡ å®Ÿè·µä¾‹

### ã‚±ãƒ¼ã‚¹1: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
```python
# çŠ¶æ³: é›‘ç„¶ã¨ã—ãŸãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚³ãƒ¼ãƒ‰

# âŒ èª­ã¿ã«ãã„å®Ÿè£…
df = pd.read_csv('data.csv')
df = df[df['age'] > 0]
df['age_group'] = pd.cut(df['age'], bins=[0, 20, 40, 60, 100])
df = df.dropna()
df = pd.get_dummies(df, columns=['category'])

# âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®šç¾©
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), ['age', 'income']),
    ('cat', OneHotEncoder(), ['category'])
])

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', RandomForestClassifier())
])

# å®Ÿè¡Œ
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)

# çµæœ: å†ç¾æ€§å‘ä¸Šã€ã‚³ãƒ¼ãƒ‰å¯èª­æ€§å‘ä¸Š
```

### ã‚±ãƒ¼ã‚¹2: å®Ÿé¨“ç®¡ç†ï¼ˆMLflowï¼‰
```python
# çŠ¶æ³: å®Ÿé¨“çµæœãŒæ•£é€¸ã€å†ç¾å›°é›£

import mlflow
import mlflow.sklearn

# âœ… MLflowã§å®Ÿé¨“ç®¡ç†
with mlflow.start_run():
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 10)

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model = RandomForestClassifier(n_estimators=100, max_depth=10)
    model.fit(X_train, y_train)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
    accuracy = model.score(X_test, y_test)
    mlflow.log_metric("accuracy", accuracy)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    mlflow.sklearn.log_model(model, "model")

    # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜ï¼ˆå›³è¡¨ç­‰ï¼‰
    plt.figure()
    plot_confusion_matrix(model, X_test, y_test)
    plt.savefig("confusion_matrix.png")
    mlflow.log_artifact("confusion_matrix.png")

# çµæœ: å®Ÿé¨“ã®å®Œå…¨ãªå†ç¾æ€§ã€æ¯”è¼ƒå®¹æ˜“
# MLflow UI ã§å…¨å®Ÿé¨“ã‚’æ¯”è¼ƒå¯èƒ½
```

### ã‚±ãƒ¼ã‚¹3: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆDaskï¼‰
```python
# çŠ¶æ³: pandas ã§100GBãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒãƒ¡ãƒ¢ãƒªä¸è¶³

import dask.dataframe as dd

# âŒ pandasï¼ˆãƒ¡ãƒ¢ãƒªä¸è¶³ï¼‰
# df = pd.read_csv('100gb_file.csv')  # MemoryError

# âœ… Daskï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
df = dd.read_csv('100gb_file.csv')

# pandas ã¨åŒã˜ API
result = df.groupby('category')['value'].mean().compute()

# ä¸¦åˆ—å‡¦ç†ã§é«˜é€ŸåŒ–
from dask.distributed import Client
client = Client(n_workers=4)  # 4ä¸¦åˆ—

# çµæœ: 100GBãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†å¯èƒ½ã€4å€é«˜é€ŸåŒ–
```

### ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³

#### ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
- **æ¬ æå€¤**: fillnaã€dropnaã€è£œå®Œï¼ˆå¹³å‡å€¤ã€ä¸­å¤®å€¤ã€KNNï¼‰
- **å¤–ã‚Œå€¤**: IQRæ³•ã€Z-scoreã€winsorization
- **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: StandardScalerã€MinMaxScalerã€RobustScaler

#### ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
- **åˆ†é¡**: accuracyã€precisionã€recallã€F1ã€ROC-AUC
- **å›å¸°**: MSEã€RMSEã€MAEã€RÂ²
- **äº¤å·®æ¤œè¨¼**: k-foldã€stratified k-foldã€time series split

#### å¯è¦–åŒ–
- **matplotlib**: åŸºæœ¬ãƒ—ãƒ­ãƒƒãƒˆ
- **seaborn**: çµ±è¨ˆçš„å¯è¦–åŒ–
- **plotly**: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–

## ğŸ”§ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯é¸æŠã‚¬ã‚¤ãƒ‰

### Python
- **é©ç”¨**: æ©Ÿæ¢°å­¦ç¿’ã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€è±Šå¯Œãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- **ç‰¹å¾´**: scikit-learnã€pandasã€numpyã€TensorFlowã€PyTorch
- **æ³¨æ„ç‚¹**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆGILåˆ¶é™ï¼‰

### R
- **é©ç”¨**: çµ±è¨ˆåˆ†æã€ã‚¢ã‚«ãƒ‡ãƒŸãƒƒã‚¯ã€å¯è¦–åŒ–é‡è¦–
- **ç‰¹å¾´**: tidyverseã€ggplot2ã€çµ±è¨ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸è±Šå¯Œ
- **æ³¨æ„ç‚¹**: æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ Python ã«åŠ£ã‚‹

### Julia
- **é©ç”¨**: é«˜æ€§èƒ½è¨ˆç®—ã€ç§‘å­¦æŠ€è¡“è¨ˆç®—
- **ç‰¹å¾´**: Python ä¸¦ã¿ã®æ›¸ãã‚„ã™ã•ã€C ä¸¦ã¿ã®é€Ÿåº¦
- **æ³¨æ„ç‚¹**: ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æˆç†Ÿåº¦

## ğŸ“š å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- **pandaså…¬å¼**: https://pandas.pydata.org/
- **scikit-learnå…¬å¼**: https://scikit-learn.org/
- **MLflowå…¬å¼**: https://mlflow.org/
- **Kaggle Learn**: https://www.kaggle.com/learn
